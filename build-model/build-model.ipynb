{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4251d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "from train import main as app_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2820efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a62cb2",
   "metadata": {},
   "source": [
    "for yaml, you should change \n",
    "- image_folder\n",
    "- root_path\n",
    "- folder\n",
    "\n",
    "Also you should create a folder `'../../experiment_log/vith14.224-bs.2048-ep.300/'` for log writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b330819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:loaded params...\n",
      "{   'data': {   'batch_size': 128,\n",
      "                'color_jitter_strength': 0.0,\n",
      "                'crop_scale': [0.3, 1.0],\n",
      "                'crop_size': 224,\n",
      "                'image_folder': './',\n",
      "                'num_workers': 10,\n",
      "                'pin_mem': True,\n",
      "                'root_path': '../data-preprocess/sample_audio',\n",
      "                'use_color_distortion': False,\n",
      "                'use_gaussian_blur': False,\n",
      "                'use_horizontal_flip': False},\n",
      "    'logging': {   'folder': '../../experiment_log/vith14.224-bs.2048-ep.300/',\n",
      "                   'write_tag': 'jepa'},\n",
      "    'mask': {   'allow_overlap': False,\n",
      "                'aspect_ratio': [0.75, 1.5],\n",
      "                'enc_mask_scale': [0.85, 1.0],\n",
      "                'min_keep': 10,\n",
      "                'num_enc_masks': 1,\n",
      "                'num_pred_masks': 4,\n",
      "                'patch_size': 14,\n",
      "                'pred_mask_scale': [0.15, 0.2]},\n",
      "    'meta': {   'copy_data': False,\n",
      "                'load_checkpoint': False,\n",
      "                'model_name': 'vit_huge',\n",
      "                'pred_depth': 12,\n",
      "                'pred_emb_dim': 384,\n",
      "                'read_checkpoint': None,\n",
      "                'use_bfloat16': True},\n",
      "    'optimization': {   'ema': [0.996, 1.0],\n",
      "                        'epochs': 300,\n",
      "                        'final_lr': 1e-06,\n",
      "                        'final_weight_decay': 0.4,\n",
      "                        'ipe_scale': 1.0,\n",
      "                        'lr': 0.001,\n",
      "                        'start_lr': 0.0002,\n",
      "                        'warmup': 40,\n",
      "                        'weight_decay': 0.04}}\n"
     ]
    }
   ],
   "source": [
    "# -- load script params\n",
    "fname = 'in1k_vith14_ep300.yaml'\n",
    "params = None\n",
    "with open(fname, 'r') as y_file:\n",
    "    params = yaml.load(y_file, Loader=yaml.FullLoader)\n",
    "    logger.info('loaded params...')\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd91c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
    "try:\n",
    "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
    "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
    "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
    "    # --          TO EACH PROCESS\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
    "from src.masks.utils import apply_masks\n",
    "from src.utils.distributed import (\n",
    "    init_distributed,\n",
    "    AllReduce\n",
    ")\n",
    "from src.utils.logging import (\n",
    "    CSVLogger,\n",
    "    gpu_timer,\n",
    "    grad_logger,\n",
    "    AverageMeter)\n",
    "from src.utils.tensors import repeat_interleave_batch\n",
    "from src.datasets.imagenet1k import make_imagenet1k\n",
    "\n",
    "from src.helper import (\n",
    "    load_checkpoint,\n",
    "    init_model,\n",
    "    init_opt)\n",
    "from src.transforms import make_transforms\n",
    "\n",
    "\n",
    "def main(args, resume_preempt=False):\n",
    "\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    #  PASSED IN PARAMS FROM CONFIG FILE\n",
    "    # ----------------------------------------------------------------------- #\n",
    "\n",
    "    # -- META\n",
    "    use_bfloat16 = args['meta']['use_bfloat16']\n",
    "    model_name = args['meta']['model_name']\n",
    "    load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
    "    r_file = args['meta']['read_checkpoint']\n",
    "    copy_data = args['meta']['copy_data']\n",
    "    pred_depth = args['meta']['pred_depth']\n",
    "    pred_emb_dim = args['meta']['pred_emb_dim']\n",
    "    if not torch.cuda.is_available():\n",
    "        device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cuda:0')\n",
    "        torch.cuda.set_device(device)\n",
    "\n",
    "    # -- DATA\n",
    "    use_gaussian_blur = args['data']['use_gaussian_blur']\n",
    "    use_horizontal_flip = args['data']['use_horizontal_flip']\n",
    "    use_color_distortion = args['data']['use_color_distortion']\n",
    "    color_jitter = args['data']['color_jitter_strength']\n",
    "    # --\n",
    "    batch_size = args['data']['batch_size']\n",
    "    pin_mem = args['data']['pin_mem']\n",
    "    num_workers = args['data']['num_workers']\n",
    "    root_path = args['data']['root_path']\n",
    "    image_folder = args['data']['image_folder']\n",
    "    crop_size = args['data']['crop_size']\n",
    "    crop_scale = args['data']['crop_scale']\n",
    "    # --\n",
    "\n",
    "    # -- MASK\n",
    "    allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
    "    patch_size = args['mask']['patch_size']  # patch-size for model training\n",
    "    num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
    "    min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
    "    enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
    "    num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
    "    pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
    "    aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
    "    # --\n",
    "\n",
    "    # -- OPTIMIZATION\n",
    "    ema = args['optimization']['ema']\n",
    "    ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
    "    wd = float(args['optimization']['weight_decay'])\n",
    "    final_wd = float(args['optimization']['final_weight_decay'])\n",
    "    num_epochs = args['optimization']['epochs']\n",
    "    warmup = args['optimization']['warmup']\n",
    "    start_lr = args['optimization']['start_lr']\n",
    "    lr = args['optimization']['lr']\n",
    "    final_lr = args['optimization']['final_lr']\n",
    "\n",
    "    # -- LOGGING\n",
    "    folder = args['logging']['folder']\n",
    "    tag = args['logging']['write_tag']\n",
    "\n",
    "    dump = os.path.join(folder, 'params-ijepa.yaml')\n",
    "    with open(dump, 'w') as f:\n",
    "        yaml.dump(args, f)\n",
    "    # ----------------------------------------------------------------------- #\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method('spawn')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # -- init torch distributed backend\n",
    "    world_size, rank = init_distributed()\n",
    "    logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
    "    if rank > 0:\n",
    "        logger.setLevel(logging.ERROR)\n",
    "\n",
    "    # -- log/checkpointing paths\n",
    "    log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
    "    save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
    "    latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
    "    load_path = None\n",
    "    if load_model:\n",
    "        load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
    "\n",
    "    # -- make csv_logger\n",
    "    csv_logger = CSVLogger(log_file,\n",
    "                           ('%d', 'epoch'),\n",
    "                           ('%d', 'itr'),\n",
    "                           ('%.5f', 'loss'),\n",
    "                           ('%.5f', 'mask-A'),\n",
    "                           ('%.5f', 'mask-B'),\n",
    "                           ('%d', 'time (ms)'))\n",
    "\n",
    "    # -- init model\n",
    "    encoder, predictor = init_model(\n",
    "        device=device,\n",
    "        patch_size=patch_size,\n",
    "        crop_size=crop_size,\n",
    "        pred_depth=pred_depth,\n",
    "        pred_emb_dim=pred_emb_dim,\n",
    "        model_name=model_name)\n",
    "    target_encoder = copy.deepcopy(encoder)\n",
    "\n",
    "    # -- make data transforms\n",
    "    mask_collator = MBMaskCollator(\n",
    "        input_size=crop_size,\n",
    "        patch_size=patch_size,\n",
    "        pred_mask_scale=pred_mask_scale,\n",
    "        enc_mask_scale=enc_mask_scale,\n",
    "        aspect_ratio=aspect_ratio,\n",
    "        nenc=num_enc_masks,\n",
    "        npred=num_pred_masks,\n",
    "        allow_overlap=allow_overlap,\n",
    "        min_keep=min_keep)\n",
    "\n",
    "    transform = make_transforms(\n",
    "        crop_size=crop_size,\n",
    "        crop_scale=crop_scale,\n",
    "        gaussian_blur=use_gaussian_blur,\n",
    "        horizontal_flip=use_horizontal_flip,\n",
    "        color_distortion=use_color_distortion,\n",
    "        color_jitter=color_jitter)\n",
    "\n",
    "    # -- init data-loaders/samplers\n",
    "    # !!! here you need to change it to your own data loader\n",
    "    _, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
    "            transform=transform,\n",
    "            batch_size=batch_size,\n",
    "            collator=mask_collator,\n",
    "            pin_mem=pin_mem,\n",
    "            training=True,\n",
    "            num_workers=num_workers,\n",
    "            world_size=world_size,\n",
    "            rank=rank,\n",
    "            root_path=root_path,\n",
    "            image_folder=image_folder,\n",
    "            copy_data=copy_data,\n",
    "            drop_last=True)\n",
    "    ipe = len(unsupervised_loader)\n",
    "\n",
    "    # -- init optimizer and scheduler\n",
    "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
    "        encoder=encoder,\n",
    "        predictor=predictor,\n",
    "        wd=wd,\n",
    "        final_wd=final_wd,\n",
    "        start_lr=start_lr,\n",
    "        ref_lr=lr,\n",
    "        final_lr=final_lr,\n",
    "        iterations_per_epoch=ipe,\n",
    "        warmup=warmup,\n",
    "        num_epochs=num_epochs,\n",
    "        ipe_scale=ipe_scale,\n",
    "        use_bfloat16=use_bfloat16)\n",
    "    encoder = DistributedDataParallel(encoder, static_graph=True)\n",
    "    predictor = DistributedDataParallel(predictor, static_graph=True)\n",
    "    target_encoder = DistributedDataParallel(target_encoder)\n",
    "    for p in target_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # -- momentum schedule\n",
    "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
    "                          for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
    "\n",
    "    start_epoch = 0\n",
    "    # -- load training checkpoint\n",
    "    if load_model:\n",
    "        encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
    "            device=device,\n",
    "            r_path=load_path,\n",
    "            encoder=encoder,\n",
    "            predictor=predictor,\n",
    "            target_encoder=target_encoder,\n",
    "            opt=optimizer,\n",
    "            scaler=scaler)\n",
    "        for _ in range(start_epoch*ipe):\n",
    "            scheduler.step()\n",
    "            wd_scheduler.step()\n",
    "            next(momentum_scheduler)\n",
    "            mask_collator.step()\n",
    "\n",
    "    def save_checkpoint(epoch):\n",
    "        save_dict = {\n",
    "            'encoder': encoder.state_dict(),\n",
    "            'predictor': predictor.state_dict(),\n",
    "            'target_encoder': target_encoder.state_dict(),\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'scaler': None if scaler is None else scaler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': loss_meter.avg,\n",
    "            'batch_size': batch_size,\n",
    "            'world_size': world_size,\n",
    "            'lr': lr\n",
    "        }\n",
    "        if rank == 0:\n",
    "            torch.save(save_dict, latest_path)\n",
    "            if (epoch + 1) % checkpoint_freq == 0:\n",
    "                torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26bef0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:SLURM vars not set (distributed training not available)\n",
      "INFO:root:Initialized (rank/world-size) 0/1\n",
      "INFO:root:VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x Block(\n",
      "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "INFO:root:making imagenet data transforms\n",
      "INFO:root:data-path ../data-preprocess/sample_audio/./train/\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data-preprocess/sample_audio/./train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m load_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m resume_preempt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m load_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m load_model\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume_preempt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_preempt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 171\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args, resume_preempt)\u001b[0m\n\u001b[1;32m    162\u001b[0m transform \u001b[38;5;241m=\u001b[39m make_transforms(\n\u001b[1;32m    163\u001b[0m     crop_size\u001b[38;5;241m=\u001b[39mcrop_size,\n\u001b[1;32m    164\u001b[0m     crop_scale\u001b[38;5;241m=\u001b[39mcrop_scale,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m     color_distortion\u001b[38;5;241m=\u001b[39muse_color_distortion,\n\u001b[1;32m    168\u001b[0m     color_jitter\u001b[38;5;241m=\u001b[39mcolor_jitter)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# -- init data-loaders/samplers\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m _, unsupervised_loader, unsupervised_sampler \u001b[38;5;241m=\u001b[39m \u001b[43mmake_imagenet1k\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpin_mem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpin_mem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m ipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(unsupervised_loader)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# -- init optimizer and scheduler\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/cva-jepa-project/cav-jepa-prepare/build-model/src/datasets/imagenet1k.py:38\u001b[0m, in \u001b[0;36mmake_imagenet1k\u001b[0;34m(transform, batch_size, collator, pin_mem, num_workers, world_size, rank, root_path, image_folder, training, copy_data, drop_last, subset_file)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_imagenet1k\u001b[39m(\n\u001b[1;32m     24\u001b[0m     transform,\n\u001b[1;32m     25\u001b[0m     batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     subset_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     37\u001b[0m ):\n\u001b[0;32m---> 38\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subset_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m ImageNetSubset(dataset, subset_file)\n",
      "File \u001b[0;32m~/Desktop/cva-jepa-project/cav-jepa-prepare/build-model/src/datasets/imagenet1k.py:109\u001b[0m, in \u001b[0;36mImageNet.__init__\u001b[0;34m(self, root, image_folder, tar_file, transform, train, job_id, local_rank, copy_data, index_targets)\u001b[0m\n\u001b[1;32m    106\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, image_folder, suffix)\n\u001b[1;32m    107\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImageNet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialized ImageNet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_targets:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data-preprocess/sample_audio/./train/'"
     ]
    }
   ],
   "source": [
    "load_model=None\n",
    "resume_preempt = False if load_model is None else load_model\n",
    "main(args=params, resume_preempt=resume_preempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9abb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e0375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
