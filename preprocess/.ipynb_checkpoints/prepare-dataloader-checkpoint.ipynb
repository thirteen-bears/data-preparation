{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6dff681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from easydict import EasyDict\n",
    "from dataloader import AudiosetDataset\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import json\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1252a838",
   "metadata": {},
   "source": [
    "To do list:\n",
    "\n",
    "I need to change `self.get_image` in AudiosetDataset.\n",
    "\n",
    "Note that `fbank = self._wav2fbank(datum['wav'], None, 0)` is correct when `datum['wav']`  indicates the correct audio data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57e339d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['data_train'] = './sample_datafiles/sample_json_subset.json'\n",
    "args['label_csv'] = './sample_datafiles/class_labels_indices_subset.csv'\n",
    "args['roll_mag_aug'] = False #use roll_mag_aug\n",
    "\n",
    "# for audio_conf \n",
    "args['freqm']  = 0  # frequency mask max length, pretraining 0\n",
    "args['timem'] = 0  # time mask max length, pretraining 0\n",
    "args['mixup'] = 0 # how many (0-1) samples need to be mixup during training\n",
    "args['dataset'] = \"audioset\"  # choices=[\"audioset\", \"esc50\", \"speechcommands\"]\n",
    "args['load_video'] = False\n",
    "args = EasyDict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ed36809",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_length = {'audioset':1024, 'esc50':512, 'speechcommands':128}\n",
    "norm_stats = {'audioset':[-4.2677393, 4.5689974], 'esc50':[-6.6268077, 5.358466], 'speechcommands':[-6.845978, 5.5654526]}\n",
    "multilabel_dataset = {'audioset': True, 'esc50': False, 'k400': False, 'speechcommands': True}\n",
    "audio_conf = {'num_mel_bins': 128, \n",
    "              'target_length': target_length[args.dataset],  # needed\n",
    "              'freqm': args.freqm,\n",
    "              'timem': args.timem,\n",
    "              'mixup': args.mixup,\n",
    "              'dataset': args.dataset,\n",
    "              'mode':'train',\n",
    "              'mean':norm_stats[args.dataset][0],\n",
    "              'std':norm_stats[args.dataset][1],\n",
    "              'multilabel':multilabel_dataset[args.dataset],\n",
    "              'noise':False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce83dc4",
   "metadata": {},
   "source": [
    "Note that dataset_json_file must have the format like this:\n",
    "\n",
    "```\n",
    "{\n",
    " \"data\": [\n",
    "  {\n",
    "   \"video_id\": \"--00W1lcxW-WU_40.000\",\n",
    "   \"wav\": \"./sample_audio/00W1lcxW-WU_40.000.wav\",\n",
    "   \"video_path\": \"./sample_frames/00W1lcxW-WU_40.000/\",\n",
    "   \"labels\": \"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\n",
    "  },\n",
    "  {\n",
    "    \"video_id\": \"--KlsG1EnBEjc_000361\",\n",
    "    \"wav\": \"./sample_audio/KlsG1EnBEjc_000361.wav\",\n",
    "    \"video_path\": \"./sample_frames/KlsG1EnBEjc_000361/\",\n",
    "    \"labels\": \"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\n",
    "   }\n",
    "  \n",
    " ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "673d9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataset_json_file, audio_conf, label_csv=None, train_model = True):\n",
    "        # load dataset\n",
    "        \n",
    "        self.datapath = dataset_json_file\n",
    "        with open(dataset_json_file, 'r') as fp:\n",
    "            data_json = json.load(fp)\n",
    "        self.data = data_json['data']\n",
    "        self.data = self.pro_data(self.data)\n",
    "        self.num_samples = self.data.shape[0]\n",
    "        print('Dataset has {:d} samples'.format(self.num_samples))\n",
    "        \n",
    "        # some parameters\n",
    "        self.audio_conf = audio_conf\n",
    "        self.melbins = self.audio_conf.get('num_mel_bins')\n",
    "        self.target_length = self.audio_conf.get('target_length')\n",
    "        self.train_model = train_model\n",
    "        self.norm_mean = self.audio_conf.get('mean')\n",
    "        self.norm_std = self.audio_conf.get('std')\n",
    "        self.freqm = self.audio_conf.get('freqm', 0)\n",
    "        self.timem = self.audio_conf.get('timem', 0)\n",
    "        print('now using following mask: {:d} freq, {:d} time'.format(self.audio_conf.get('freqm'), self.audio_conf.get('timem')))\n",
    "        \n",
    "        # if add noise for data augmentation\n",
    "        self.noise = self.audio_conf.get('noise', False)\n",
    "        if self.noise == True:\n",
    "            print('now use noise augmentation')\n",
    "        else:\n",
    "            print('not use noise augmentation')\n",
    "        \n",
    "        # skip_norm is a flag that if you want to skip normalization to compute the normalization stats using src/get_norm_stats.py, if Ture, input normalization will be skipped for correctly calculating the stats.\n",
    "        # set it as True ONLY when you are getting the normalization stats.\n",
    "        self.skip_norm = self.audio_conf.get('skip_norm') if self.audio_conf.get('skip_norm') else False\n",
    "        if self.skip_norm:\n",
    "            print('now skip normalization (use it ONLY when you are computing the normalization stats).')\n",
    "        else:\n",
    "            print('use dataset mean {:.3f} and std {:.3f} to normalize the input.'.format(self.norm_mean, self.norm_std))\n",
    "    \n",
    "    # change python list to numpy array to avoid memory leak.\n",
    "    def pro_data(self, data_json):\n",
    "        for i in range(len(data_json)):\n",
    "            data_json[i] = [data_json[i]['wav'], data_json[i]['labels'], data_json[i]['video_id'], data_json[i]['video_path']]\n",
    "        data_np = np.array(data_json, dtype=str)\n",
    "        return data_np\n",
    "    \n",
    "    def decode_data(self, np_data):\n",
    "        datum = {}\n",
    "        datum['wav'] = np_data[0]\n",
    "        datum['labels'] = np_data[1]\n",
    "        datum['video_id'] = np_data[2]\n",
    "        datum['video_path'] = np_data[3]\n",
    "        return datum\n",
    "    \n",
    "    def _wav2fbank(self, filename):\n",
    "        waveform, sr = torchaudio.load(filename)\n",
    "        waveform = waveform - waveform.mean()\n",
    "        try:\n",
    "            fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False, window_type='hanning', num_mel_bins=self.melbins, dither=0.0, frame_shift=10)\n",
    "        except:\n",
    "            fbank = torch.zeros([512, 128]) + 0.01\n",
    "            print('there is a loading error')  \n",
    "        target_length = self.target_length\n",
    "        n_frames = fbank.shape[0]\n",
    "        p = target_length - n_frames\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "        return fbank\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        datum = self.data[index]\n",
    "        datum = self.decode_data(datum) # to jason file\n",
    "        \n",
    "        try:\n",
    "            fbank = self._wav2fbank(datum['wav'])\n",
    "        except:\n",
    "            fbank = torch.zeros([self.target_length, 128]) + 0.01\n",
    "            print('there is an error in loading audio')\n",
    "        \n",
    "\n",
    "        # SpecAug, not do for eval set\n",
    "        if self.train_model: # training mode\n",
    "            freqm = torchaudio.transforms.FrequencyMasking(self.freqm)\n",
    "            timem = torchaudio.transforms.TimeMasking(self.timem)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "            fbank = fbank.unsqueeze(0)\n",
    "            if self.freqm != 0:\n",
    "                fbank = freqm(fbank)\n",
    "            if self.timem != 0:\n",
    "                fbank = timem(fbank)\n",
    "            fbank = fbank.squeeze(0)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "            # normalize the input for both training and test\n",
    "            if self.skip_norm == False:\n",
    "                fbank = (fbank - self.norm_mean) / (self.norm_std)\n",
    "            # skip normalization the input ONLY when you are trying to get the normalization stats.\n",
    "            else:\n",
    "                pass\n",
    "            # if add noise for data augmentation\n",
    "            #if (self.noise == True) and (self.train_model == True):\n",
    "            if self.noise == True :\n",
    "                fbank = fbank + torch.rand(fbank.shape[0], fbank.shape[1]) * np.random.rand() / 10\n",
    "                fbank = torch.roll(fbank, np.random.randint(-self.target_length, self.target_length), 0)\n",
    "        \n",
    "        else: #evaluation mode\n",
    "            fbank = fbank.squeeze(0)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "\n",
    "            # normalize the input for both training and test\n",
    "            if self.skip_norm == False:\n",
    "                fbank = (fbank - self.norm_mean) / (self.norm_std)\n",
    "            # skip normalization the input ONLY when you are trying to get the normalization stats.\n",
    "            else:\n",
    "                pass\n",
    "        return fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b275640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2 samples\n",
      "now using following mask: 0 freq, 0 time\n",
      "not use noise augmentation\n",
      "use dataset mean -4.268 and std 4.569 to normalize the input.\n"
     ]
    }
   ],
   "source": [
    "dataset_train = AudioDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf, \n",
    "                                #roll_mag_aug=args.roll_mag_aug,\n",
    "                                #load_video=args.load_video\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c646cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = iter(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "214225b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3889, -1.0507, -0.2970,  ..., -0.3272, -0.4664, -0.7628],\n",
       "        [-0.8725, -1.0141, -0.2604,  ..., -0.5665, -0.4234, -0.7432],\n",
       "        [-0.7921, -1.1173, -0.3637,  ...,  0.3071,  0.2998, -0.0308],\n",
       "        ...,\n",
       "        [ 0.9341,  0.9341,  0.9341,  ...,  0.9341,  0.9341,  0.9341],\n",
       "        [ 0.9341,  0.9341,  0.9341,  ...,  0.9341,  0.9341,  0.9341],\n",
       "        [ 0.9341,  0.9341,  0.9341,  ...,  0.9341,  0.9341,  0.9341]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ac01a",
   "metadata": {},
   "source": [
    "### 1.2 one simple test to view audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c7ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2fbank(filename):\n",
    "        waveform, sr = torchaudio.load(filename)\n",
    "        waveform = waveform - waveform.mean()\n",
    "        fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False, window_type='hanning', \n",
    "                                                      num_mel_bins=128, dither=0.0, frame_shift=10)\n",
    "  \n",
    "        target_length = 1024\n",
    "        n_frames = fbank.shape[0]\n",
    "        p = target_length - n_frames\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "        return fbank\n",
    "    \n",
    "path = \"./sample_audio/00W1lcxW-WU_40.000.wav\"\n",
    "fbank = wav2fbank(path)\n",
    "fbank "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
