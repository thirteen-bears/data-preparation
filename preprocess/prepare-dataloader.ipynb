{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f6dff681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from easydict import EasyDict\n",
    "from dataloader import AudiosetDataset\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb2c755",
   "metadata": {},
   "source": [
    "To do list:\n",
    "\n",
    "I need to change `self.get_image` in AudiosetDataset.\n",
    "\n",
    "Note that `fbank = self._wav2fbank(datum['wav'], None, 0)` is correct when `datum['wav']`  indicates the correct audio data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bafaf2",
   "metadata": {},
   "source": [
    "# 1. Audio dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d259fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['data_train'] = './sample_datafiles/sample_json_subset.json'\n",
    "args['label_csv'] = './sample_datafiles/class_labels_indices_subset.csv'\n",
    "args['roll_mag_aug'] = False #use roll_mag_aug\n",
    "\n",
    "# for audio_conf \n",
    "args['freqm']  = 0  # frequency mask max length, pretraining 0\n",
    "args['timem'] = 0  # time mask max length, pretraining 0\n",
    "args['mixup'] = 0 # how many (0-1) samples need to be mixup during training\n",
    "args['dataset'] = \"audioset\"  # choices=[\"audioset\", \"esc50\", \"speechcommands\"]\n",
    "args['load_video'] = False\n",
    "args = EasyDict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34c857c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_length = {'audioset':1024, 'esc50':512, 'speechcommands':128}\n",
    "norm_stats = {'audioset':[-4.2677393, 4.5689974], 'esc50':[-6.6268077, 5.358466], 'speechcommands':[-6.845978, 5.5654526]}\n",
    "multilabel_dataset = {'audioset': True, 'esc50': False, 'k400': False, 'speechcommands': True}\n",
    "audio_conf = {'num_mel_bins': 128, \n",
    "              'target_length': target_length[args.dataset],  # needed\n",
    "              'freqm': args.freqm,\n",
    "              'timem': args.timem,\n",
    "              'mixup': args.mixup,\n",
    "              'dataset': args.dataset,\n",
    "              'mode':'train',\n",
    "              'mean':norm_stats[args.dataset][0],\n",
    "              'std':norm_stats[args.dataset][1],\n",
    "              'multilabel':multilabel_dataset[args.dataset],\n",
    "              'noise':False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed4c89",
   "metadata": {},
   "source": [
    "Note that dataset_json_file must have the format like this:\n",
    "\n",
    "```\n",
    "{\n",
    " \"data\": [\n",
    "  {\n",
    "   \"video_id\": \"--00W1lcxW-WU_40.000\",\n",
    "   \"wav\": \"./sample_audio/00W1lcxW-WU_40.000.wav\",\n",
    "   \"video_path\": \"./sample_frames/00W1lcxW-WU_40.000/\",\n",
    "   \"labels\": \"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\n",
    "  },\n",
    "  {\n",
    "    \"video_id\": \"--KlsG1EnBEjc_000361\",\n",
    "    \"wav\": \"./sample_audio/KlsG1EnBEjc_000361.wav\",\n",
    "    \"video_path\": \"./sample_frames/KlsG1EnBEjc_000361/\",\n",
    "    \"labels\": \"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\n",
    "   }\n",
    "  \n",
    " ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "49d41137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataset_json_file, audio_conf, label_csv=None, train_model = True):\n",
    "        # load dataset\n",
    "        \n",
    "        self.datapath = dataset_json_file\n",
    "        with open(dataset_json_file, 'r') as fp:\n",
    "            data_json = json.load(fp)\n",
    "        self.data = data_json['data']\n",
    "        self.data = self.pro_data(self.data)\n",
    "        self.num_samples = self.data.shape[0]\n",
    "        print('Dataset has {:d} samples'.format(self.num_samples))\n",
    "        \n",
    "        # some parameters\n",
    "        self.audio_conf = audio_conf\n",
    "        self.melbins = self.audio_conf.get('num_mel_bins')\n",
    "        self.target_length = self.audio_conf.get('target_length')\n",
    "        self.train_model = train_model\n",
    "        self.norm_mean = self.audio_conf.get('mean')\n",
    "        self.norm_std = self.audio_conf.get('std')\n",
    "        self.freqm = self.audio_conf.get('freqm', 0)\n",
    "        self.timem = self.audio_conf.get('timem', 0)\n",
    "        print('now using following mask: {:d} freq, {:d} time'.format(self.audio_conf.get('freqm'), self.audio_conf.get('timem')))\n",
    "        \n",
    "        # if add noise for data augmentation\n",
    "        self.noise = self.audio_conf.get('noise', False)\n",
    "        if self.noise == True:\n",
    "            print('now use noise augmentation')\n",
    "        else:\n",
    "            print('not use noise augmentation')\n",
    "        \n",
    "        # skip_norm is a flag that if you want to skip normalization to compute the normalization stats using src/get_norm_stats.py, if Ture, input normalization will be skipped for correctly calculating the stats.\n",
    "        # set it as True ONLY when you are getting the normalization stats.\n",
    "        self.skip_norm = self.audio_conf.get('skip_norm') if self.audio_conf.get('skip_norm') else False\n",
    "        if self.skip_norm:\n",
    "            print('now skip normalization (use it ONLY when you are computing the normalization stats).')\n",
    "        else:\n",
    "            print('use dataset mean {:.3f} and std {:.3f} to normalize the input.'.format(self.norm_mean, self.norm_std))\n",
    "    \n",
    "    # change python list to numpy array to avoid memory leak.\n",
    "    def pro_data(self, data_json):\n",
    "        for i in range(len(data_json)):\n",
    "            data_json[i] = [data_json[i]['wav'], data_json[i]['labels'], data_json[i]['video_id'], data_json[i]['video_path']]\n",
    "        data_np = np.array(data_json, dtype=str)\n",
    "        return data_np\n",
    "    \n",
    "    def decode_data(self, np_data):\n",
    "        datum = {}\n",
    "        datum['wav'] = np_data[0]\n",
    "        datum['labels'] = np_data[1]\n",
    "        datum['video_id'] = np_data[2]\n",
    "        datum['video_path'] = np_data[3]\n",
    "        return datum\n",
    "    \n",
    "    def _wav2fbank(self, filename):\n",
    "        waveform, sr = torchaudio.load(filename)\n",
    "        waveform = waveform - waveform.mean()\n",
    "        try:\n",
    "            fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False, window_type='hanning', num_mel_bins=self.melbins, dither=0.0, frame_shift=10)\n",
    "        except:\n",
    "            fbank = torch.zeros([512, 128]) + 0.01\n",
    "            print('there is a loading error')  \n",
    "        target_length = self.target_length\n",
    "        n_frames = fbank.shape[0]\n",
    "        p = target_length - n_frames\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "        return fbank\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        datum = self.data[index]\n",
    "        datum = self.decode_data(datum) # to jason file\n",
    "        \n",
    "        try:\n",
    "            fbank = self._wav2fbank(datum['wav'])\n",
    "        except:\n",
    "            fbank = torch.zeros([self.target_length, 128]) + 0.01\n",
    "            print('there is an error in loading audio')\n",
    "        \n",
    "\n",
    "        # SpecAug, not do for eval set\n",
    "        if self.train_model: # training mode\n",
    "            freqm = torchaudio.transforms.FrequencyMasking(self.freqm)\n",
    "            timem = torchaudio.transforms.TimeMasking(self.timem)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "            fbank = fbank.unsqueeze(0)\n",
    "            if self.freqm != 0:\n",
    "                fbank = freqm(fbank)\n",
    "            if self.timem != 0:\n",
    "                fbank = timem(fbank)\n",
    "            fbank = fbank.squeeze(0)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "            # normalize the input for both training and test\n",
    "            if self.skip_norm == False:\n",
    "                fbank = (fbank - self.norm_mean) / (self.norm_std)\n",
    "            # skip normalization the input ONLY when you are trying to get the normalization stats.\n",
    "            else:\n",
    "                pass\n",
    "            # if add noise for data augmentation\n",
    "            #if (self.noise == True) and (self.train_model == True):\n",
    "            if self.noise == True :\n",
    "                fbank = fbank + torch.rand(fbank.shape[0], fbank.shape[1]) * np.random.rand() / 10\n",
    "                fbank = torch.roll(fbank, np.random.randint(-self.target_length, self.target_length), 0)\n",
    "        \n",
    "        else: #evaluation mode\n",
    "            fbank = fbank.squeeze(0)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "\n",
    "            # normalize the input for both training and test\n",
    "            if self.skip_norm == False:\n",
    "                fbank = (fbank - self.norm_mean) / (self.norm_std)\n",
    "            # skip normalization the input ONLY when you are trying to get the normalization stats.\n",
    "            else:\n",
    "                pass\n",
    "        return fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e3532c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2 samples\n",
      "now using following mask: 0 freq, 0 time\n",
      "not use noise augmentation\n",
      "use dataset mean -4.268 and std 4.569 to normalize the input.\n"
     ]
    }
   ],
   "source": [
    "dataset_train = AudioDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf, \n",
    "                                #roll_mag_aug=args.roll_mag_aug,\n",
    "                                #load_video=args.load_video\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3dce889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = iter(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6fad26bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3889, -1.0507, -0.2970,  ..., -0.3272, -0.4664, -0.7628],\n",
       "        [-0.8725, -1.0141, -0.2604,  ..., -0.5665, -0.4234, -0.7432],\n",
       "        [-0.7921, -1.1173, -0.3637,  ...,  0.3071,  0.2998, -0.0308],\n",
       "        ...,\n",
       "        [ 0.9341,  0.9341,  0.9341,  ...,  0.9341,  0.9341,  0.9341],\n",
       "        [ 0.9341,  0.9341,  0.9341,  ...,  0.9341,  0.9341,  0.9341],\n",
       "        [ 0.9341,  0.9341,  0.9341,  ...,  0.9341,  0.9341,  0.9341]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b014bb2",
   "metadata": {},
   "source": [
    "### 1.2 one simple test to load audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "73ec85e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.6136,  -9.0683,  -5.6249,  ...,  -5.7625,  -6.3989,  -7.7529],\n",
       "        [ -8.2543,  -8.9011,  -5.4577,  ...,  -6.8562,  -6.2021,  -7.6635],\n",
       "        [ -7.8870,  -9.3728,  -5.9294,  ...,  -2.8646,  -2.8981,  -4.4085],\n",
       "        ...,\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wav2fbank(filename):\n",
    "        waveform, sr = torchaudio.load(filename)\n",
    "        waveform = waveform - waveform.mean()\n",
    "        fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False, window_type='hanning', \n",
    "                                                      num_mel_bins=128, dither=0.0, frame_shift=10)\n",
    "  \n",
    "        target_length = 1024\n",
    "        n_frames = fbank.shape[0]\n",
    "        p = target_length - n_frames\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "        return fbank\n",
    "    \n",
    "path = \"./sample_audio/00W1lcxW-WU_40.000.wav\"\n",
    "fbank = wav2fbank(path)\n",
    "fbank "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d0cfd",
   "metadata": {},
   "source": [
    "# 2. Video dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1c027",
   "metadata": {},
   "source": [
    "Do we need this part?\n",
    "```\n",
    "## apply different color jittering for each frame in the video clip\n",
    "        trans_clip_cj = []\n",
    "        for frame in trans_clip:\n",
    "            frame = self.toPIL(frame)  # PIL image\n",
    "            frame = self.color_jitter_(frame)  # tensor [C x H x W]\n",
    "            frame = np.array(frame)\n",
    "            trans_clip_cj.append(frame)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c5666bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "    def __call__(self, video_clip):\n",
    "        if random.random() < self.p:\n",
    "            # t x h x w\n",
    "            #print(\"flip\")\n",
    "            flip_video_clip = np.flip(video_clip, axis=2).copy()\n",
    "            return flip_video_clip\n",
    "        return video_clip\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "    def __call__(self, video_clip):\n",
    "        h, w = video_clip.shape[1:3]\n",
    "        new_h, new_w = self.output_size\n",
    "        h_start = random.randint(0, h-new_h)\n",
    "        w_start = random.randint(0, w-new_w)\n",
    "        rnd_crop_video_clip = video_clip[:, h_start:h_start+new_h,\n",
    "                                 w_start:w_start+new_w, :]\n",
    "        return rnd_crop_video_clip\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, video_clip):\n",
    "        h, w = video_clip.shape[1:3]\n",
    "        new_h, new_w = self.output_size\n",
    "        h_start = int((h - new_h) / 2)\n",
    "        w_start = int((w- new_w) / 2)\n",
    "        center_crop_video_clip = video_clip[:, h_start:h_start + new_h,\n",
    "                                    w_start:w_start + new_w, :]\n",
    "        return center_crop_video_clip\n",
    "\n",
    "class ClipResize(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, video_clip):\n",
    "        rsz_video_clip = []\n",
    "        new_h, new_w = self.output_size\n",
    "        for frame in video_clip:\n",
    "            rsz_frame = cv2.resize(frame, (new_w, new_h))\n",
    "            rsz_video_clip.append(rsz_frame)\n",
    "        return np.array(rsz_video_clip)\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"\n",
    "    change input channel\n",
    "    D x H x W x C ---> C x D x H x w\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ToTensor, self).__init__()\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        video_clip = sample\n",
    "        video_clip = np.transpose(video_clip, (3, 0, 1, 2))\n",
    "        return video_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8286547",
   "metadata": {},
   "source": [
    "Note that dataset_json_file must have the format like this:\n",
    "\n",
    "```\n",
    "{\n",
    " \"data\": [\n",
    "  {\n",
    "   \"video_id\": \"--00W1lcxW-WU_40.000\",\n",
    "   \"wav\": \"./sample_audio/00W1lcxW-WU_40.000.wav\",\n",
    "   \"video_path\": \"./sample_frames/00W1lcxW-WU_40.000/\",\n",
    "   \"labels\": \"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\n",
    "  },\n",
    "  {\n",
    "    \"video_id\": \"--KlsG1EnBEjc_000361\",\n",
    "    \"wav\": \"./sample_audio/KlsG1EnBEjc_000361.wav\",\n",
    "    \"video_path\": \"./sample_frames/KlsG1EnBEjc_000361/\",\n",
    "    \"labels\": \"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\n",
    "   }\n",
    "  \n",
    " ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fc5a344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, dataset_json_file, clip_len = 16, transforms_=None, color_jitter_=None):\n",
    "        # load dataset\n",
    "        self.datapath = dataset_json_file\n",
    "        with open(dataset_json_file, 'r') as fp:\n",
    "            data_json = json.load(fp)\n",
    "        self.data = data_json['data']\n",
    "        self.data = self.pro_data(self.data)\n",
    "        self.num_samples = self.data.shape[0]\n",
    "        print('Dataset has {:d} samples'.format(self.num_samples))\n",
    "        \n",
    "        # some parameters\n",
    "        self.clip_len = clip_len\n",
    "        self.toPIL = transforms.ToPILImage()\n",
    "        self.transforms_ = transforms_\n",
    "        self.color_jitter_ = color_jitter_\n",
    "        \n",
    "        \n",
    "    # change python list to numpy array to avoid memory leak.\n",
    "    def pro_data(self, data_json):\n",
    "        for i in range(len(data_json)):\n",
    "            data_json[i] = [data_json[i]['wav'], data_json[i]['labels'], data_json[i]['video_id'], data_json[i]['video_path']]\n",
    "        data_np = np.array(data_json, dtype=str)\n",
    "        return data_np\n",
    "    \n",
    "    def decode_data(self, np_data):\n",
    "        datum = {}\n",
    "        datum['wav'] = np_data[0]\n",
    "        datum['labels'] = np_data[1]\n",
    "        datum['video_id'] = np_data[2]\n",
    "        datum['video_path'] = np_data[3]\n",
    "        return datum\n",
    "    \n",
    "    def _loop_load_rgb(self, video_path, clip_len):\n",
    "        video_clip = []\n",
    "        for i in range(clip_len):\n",
    "            cur_img_path = os.path.join(video_path, \"frame_\" + \"{:02}.jpg\".format(i))\n",
    "            img = cv2.imread(cur_img_path)\n",
    "            video_clip.append(img)\n",
    "        video_clip = np.array(video_clip)\n",
    "        return video_clip\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        datum = self.data[index]\n",
    "        datum = self.decode_data(datum) # to jason file\n",
    "        video_id, video_path = datum['video_id'],datum['video_path'] \n",
    "        print(video_path)\n",
    "        print(self.clip_len)\n",
    "        rgb_clip = self._loop_load_rgb(video_path, self.clip_len)\n",
    "        print(rgb_clip.shape)\n",
    "        \n",
    "        if \n",
    "        if \n",
    "        trans_clip = self.transforms_(rgb_clip)\n",
    "        \n",
    "        trans_clip_cj = trans_clip\n",
    "        ## apply different color jittering for each frame in the video clip\n",
    "        #trans_clip_cj = []\n",
    "        #for frame in trans_clip:\n",
    "            #frame = self.toPIL(frame)  # PIL image\n",
    "            #frame = self.color_jitter_(frame)  # tensor [C x H x W]\n",
    "            #frame = np.array(frame)\n",
    "            #trans_clip_cj.append(frame)\n",
    "        \n",
    "        trans_clip_cj = np.array(trans_clip_cj).transpose(3, 0, 1, 2)\n",
    "        return trans_clip_cj\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cee9b492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2 samples\n",
      "./sample_frames/00W1lcxW-WU_40.000/\n",
      "16\n",
      "(16, 224, 224, 3)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m VideoDataset(args\u001b[38;5;241m.\u001b[39mdata_train, clip_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m      2\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(dataset_train)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[119], line 52\u001b[0m, in \u001b[0;36mVideoDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     49\u001b[0m rgb_clip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_load_rgb(video_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_len)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(rgb_clip\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 52\u001b[0m trans_clip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_clip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m trans_clip_cj \u001b[38;5;241m=\u001b[39m trans_clip\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m## apply different color jittering for each frame in the video clip\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#trans_clip_cj = []\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#for frame in trans_clip:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m#frame = np.array(frame)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m#trans_clip_cj.append(frame)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "dataset_train = VideoDataset(args.data_train, clip_len = 16)\n",
    "temp = iter(dataset_train)\n",
    "next(temp).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b6ae5",
   "metadata": {},
   "source": [
    "### 2.2 one simple test to load video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a5c35b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = \"--00W1lcxW-WU_40.000\"\n",
    "video_path = \"./sample_frames/00W1lcxW-WU_40.000/\"\n",
    "clip_len = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f09b7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_load_rgb(video_path, clip_len):\n",
    "    video_clip = []\n",
    "    for i in range(clip_len):\n",
    "        cur_img_path = os.path.join(video_path, \"frame_\" + \"{:02}.jpg\".format(i))\n",
    "        img = cv2.imread(cur_img_path)\n",
    "        video_clip.append(img)\n",
    "    video_clip = np.array(video_clip)\n",
    "    return video_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6152251e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 224, 224, 3)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_clip = loop_load_rgb(video_path, clip_len)\n",
    "video_clip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84da7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
