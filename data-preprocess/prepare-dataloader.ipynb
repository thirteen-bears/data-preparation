{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6dff681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from easydict import EasyDict\n",
    "from dataloader import AudiosetDataset\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5716cc",
   "metadata": {},
   "source": [
    "To do list:\n",
    "\n",
    "I need to change `self.get_image` in AudiosetDataset.\n",
    "\n",
    "Note that `fbank = self._wav2fbank(datum['wav'], None, 0)` is correct when `datum['wav']`  indicates the correct audio data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b471936c",
   "metadata": {},
   "source": [
    "# 1. Audio dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926b5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['data_train'] = './sample_datafiles/sample_json_subset.json'\n",
    "args['label_csv'] = './sample_datafiles/class_labels_indices_subset.csv'\n",
    "args['roll_mag_aug'] = False #use roll_mag_aug\n",
    "\n",
    "# for audio_conf \n",
    "args['freqm']  = 0  # frequency mask max length, pretraining 0\n",
    "args['timem'] = 0  # time mask max length, pretraining 0\n",
    "args['mixup'] = 0 # how many (0-1) samples need to be mixup during training\n",
    "args['dataset'] = \"audioset\"  # choices=[\"audioset\", \"esc50\", \"speechcommands\"]\n",
    "args['load_video'] = False\n",
    "args = EasyDict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e69a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_length = {'audioset':1024, 'esc50':512, 'speechcommands':128}\n",
    "norm_stats = {'audioset':[-4.2677393, 4.5689974], 'esc50':[-6.6268077, 5.358466], 'speechcommands':[-6.845978, 5.5654526]}\n",
    "multilabel_dataset = {'audioset': True, 'esc50': False, 'k400': False, 'speechcommands': True}\n",
    "audio_conf = {'num_mel_bins': 128, \n",
    "              'target_length': target_length[args.dataset],  # needed\n",
    "              'freqm': args.freqm,\n",
    "              'timem': args.timem,\n",
    "              'mixup': args.mixup,\n",
    "              'dataset': args.dataset,\n",
    "              'mode':'train',\n",
    "              'mean':norm_stats[args.dataset][0],\n",
    "              'std':norm_stats[args.dataset][1],\n",
    "              'multilabel':multilabel_dataset[args.dataset],\n",
    "              'noise':False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22bb234",
   "metadata": {},
   "source": [
    "Note that dataset_json_file must have the format like this:\n",
    "\n",
    "```\n",
    "{\n",
    " \"data\": [\n",
    "  {\n",
    "   \"video_id\": \"--00W1lcxW-WU_40.000\",\n",
    "   \"wav\": \"./sample_audio/00W1lcxW-WU_40.000.wav\",\n",
    "   \"video_path\": \"./sample_frames/00W1lcxW-WU_40.000/\",\n",
    "   \"labels\": \"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\n",
    "  },\n",
    "  {\n",
    "    \"video_id\": \"--KlsG1EnBEjc_000361\",\n",
    "    \"wav\": \"./sample_audio/KlsG1EnBEjc_000361.wav\",\n",
    "    \"video_path\": \"./sample_frames/KlsG1EnBEjc_000361/\",\n",
    "    \"labels\": \"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\n",
    "   }\n",
    "  \n",
    " ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fcbdb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataset_json_file, audio_conf, label_csv=None, train_model = True):\n",
    "        # load dataset\n",
    "        \n",
    "        self.datapath = dataset_json_file\n",
    "        with open(dataset_json_file, 'r') as fp:\n",
    "            data_json = json.load(fp)\n",
    "        self.data = data_json['data']\n",
    "        self.data = self.pro_data(self.data)\n",
    "        self.num_samples = self.data.shape[0]\n",
    "        print('Dataset has {:d} samples'.format(self.num_samples))\n",
    "        \n",
    "        # some parameters\n",
    "        self.audio_conf = audio_conf\n",
    "        self.melbins = self.audio_conf.get('num_mel_bins')\n",
    "        self.target_length = self.audio_conf.get('target_length')\n",
    "        self.train_model = train_model\n",
    "        self.norm_mean = self.audio_conf.get('mean')\n",
    "        self.norm_std = self.audio_conf.get('std')\n",
    "        self.freqm = self.audio_conf.get('freqm', 0)\n",
    "        self.timem = self.audio_conf.get('timem', 0)\n",
    "        print('now using following mask: {:d} freq, {:d} time'.format(self.audio_conf.get('freqm'), self.audio_conf.get('timem')))\n",
    "        \n",
    "        # if add noise for data augmentation\n",
    "        self.noise = self.audio_conf.get('noise', False)\n",
    "        if self.noise == True:\n",
    "            print('now use noise augmentation')\n",
    "        else:\n",
    "            print('not use noise augmentation')\n",
    "        \n",
    "        # skip_norm is a flag that if you want to skip normalization to compute the normalization stats using src/get_norm_stats.py, if Ture, input normalization will be skipped for correctly calculating the stats.\n",
    "        # set it as True ONLY when you are getting the normalization stats.\n",
    "        self.skip_norm = self.audio_conf.get('skip_norm') if self.audio_conf.get('skip_norm') else False\n",
    "        if self.skip_norm:\n",
    "            print('now skip normalization (use it ONLY when you are computing the normalization stats).')\n",
    "        else:\n",
    "            print('use dataset mean {:.3f} and std {:.3f} to normalize the input.'.format(self.norm_mean, self.norm_std))\n",
    "    \n",
    "    # change python list to numpy array to avoid memory leak.\n",
    "    def pro_data(self, data_json):\n",
    "        for i in range(len(data_json)):\n",
    "            data_json[i] = [data_json[i]['wav'], data_json[i]['labels'], data_json[i]['video_id'], data_json[i]['video_path']]\n",
    "        data_np = np.array(data_json, dtype=str)\n",
    "        return data_np\n",
    "    \n",
    "    def decode_data(self, np_data):\n",
    "        datum = {}\n",
    "        datum['wav'] = np_data[0]\n",
    "        datum['labels'] = np_data[1]\n",
    "        datum['video_id'] = np_data[2]\n",
    "        datum['video_path'] = np_data[3]\n",
    "        return datum\n",
    "    \n",
    "    def _wav2fbank(self, filename):\n",
    "        waveform, sr = torchaudio.load(filename)\n",
    "        waveform = waveform - waveform.mean()\n",
    "        try:\n",
    "            fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False, window_type='hanning', num_mel_bins=self.melbins, dither=0.0, frame_shift=10)\n",
    "        except:\n",
    "            fbank = torch.zeros([512, 128]) + 0.01\n",
    "            print('there is a loading error')  \n",
    "        target_length = self.target_length\n",
    "        n_frames = fbank.shape[0]\n",
    "        p = target_length - n_frames\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "        return fbank\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        datum = self.data[index]\n",
    "        datum = self.decode_data(datum) # to jason file\n",
    "        \n",
    "        try:\n",
    "            fbank = self._wav2fbank(datum['wav'])\n",
    "        except:\n",
    "            fbank = torch.zeros([self.target_length, 128]) + 0.01\n",
    "            print('there is an error in loading audio')\n",
    "        \n",
    "\n",
    "        # SpecAug, not do for eval set\n",
    "        if self.train_model: # training mode\n",
    "            freqm = torchaudio.transforms.FrequencyMasking(self.freqm)\n",
    "            timem = torchaudio.transforms.TimeMasking(self.timem)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "            fbank = fbank.unsqueeze(0)\n",
    "            if self.freqm != 0:\n",
    "                fbank = freqm(fbank)\n",
    "            if self.timem != 0:\n",
    "                fbank = timem(fbank)\n",
    "            fbank = fbank.squeeze(0)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "            # normalize the input for both training and test\n",
    "            if self.skip_norm == False:\n",
    "                fbank = (fbank - self.norm_mean) / (self.norm_std)\n",
    "            # skip normalization the input ONLY when you are trying to get the normalization stats.\n",
    "            else:\n",
    "                pass\n",
    "            # if add noise for data augmentation\n",
    "            #if (self.noise == True) and (self.train_model == True):\n",
    "            if self.noise == True :\n",
    "                fbank = fbank + torch.rand(fbank.shape[0], fbank.shape[1]) * np.random.rand() / 10\n",
    "                fbank = torch.roll(fbank, np.random.randint(-self.target_length, self.target_length), 0)\n",
    "        \n",
    "        else: #evaluation mode\n",
    "            fbank = fbank.squeeze(0)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "\n",
    "            # normalize the input for both training and test\n",
    "            if self.skip_norm == False:\n",
    "                fbank = (fbank - self.norm_mean) / (self.norm_std)\n",
    "            # skip normalization the input ONLY when you are trying to get the normalization stats.\n",
    "            else:\n",
    "                pass\n",
    "        return fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d78ea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2 samples\n",
      "now using following mask: 0 freq, 0 time\n",
      "not use noise augmentation\n",
      "use dataset mean -4.268 and std 4.569 to normalize the input.\n"
     ]
    }
   ],
   "source": [
    "dataset_train = AudioDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf, \n",
    "                                #roll_mag_aug=args.roll_mag_aug,\n",
    "                                #load_video=args.load_video\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "641579c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = iter(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c65871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(temp).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068c711",
   "metadata": {},
   "source": [
    "### 1.2 one simple test to load audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d79eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.6136,  -9.0683,  -5.6249,  ...,  -5.7625,  -6.3989,  -7.7529],\n",
       "        [ -8.2543,  -8.9011,  -5.4577,  ...,  -6.8562,  -6.2021,  -7.6635],\n",
       "        [ -7.8870,  -9.3728,  -5.9294,  ...,  -2.8646,  -2.8981,  -4.4085],\n",
       "        ...,\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wav2fbank(filename):\n",
    "        waveform, sr = torchaudio.load(filename)\n",
    "        waveform = waveform - waveform.mean()\n",
    "        fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False, window_type='hanning', \n",
    "                                                      num_mel_bins=128, dither=0.0, frame_shift=10)\n",
    "  \n",
    "        target_length = 1024\n",
    "        n_frames = fbank.shape[0]\n",
    "        p = target_length - n_frames\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "        return fbank\n",
    "    \n",
    "path = \"./sample_audio/00W1lcxW-WU_40.000.wav\"\n",
    "fbank = wav2fbank(path)\n",
    "fbank "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d3ab4",
   "metadata": {},
   "source": [
    "# 2. Video dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddebe5",
   "metadata": {},
   "source": [
    "Do we need this part?\n",
    "```\n",
    "## apply different color jittering for each frame in the video clip\n",
    "        trans_clip_cj = []\n",
    "        for frame in trans_clip:\n",
    "            frame = self.toPIL(frame)  # PIL image\n",
    "            frame = self.color_jitter_(frame)  # tensor [C x H x W]\n",
    "            frame = np.array(frame)\n",
    "            trans_clip_cj.append(frame)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "642982ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "    def __call__(self, video_clip):\n",
    "        if random.random() < self.p:\n",
    "            # t x h x w\n",
    "            #print(\"flip\")\n",
    "            flip_video_clip = np.flip(video_clip, axis=2).copy()\n",
    "            return flip_video_clip\n",
    "        return video_clip\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "    def __call__(self, video_clip):\n",
    "        h, w = video_clip.shape[1:3]\n",
    "        new_h, new_w = self.output_size\n",
    "        h_start = random.randint(0, h-new_h)\n",
    "        w_start = random.randint(0, w-new_w)\n",
    "        rnd_crop_video_clip = video_clip[:, h_start:h_start+new_h,\n",
    "                                 w_start:w_start+new_w, :]\n",
    "        return rnd_crop_video_clip\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, video_clip):\n",
    "        h, w = video_clip.shape[1:3]\n",
    "        new_h, new_w = self.output_size\n",
    "        h_start = int((h - new_h) / 2)\n",
    "        w_start = int((w- new_w) / 2)\n",
    "        center_crop_video_clip = video_clip[:, h_start:h_start + new_h,\n",
    "                                    w_start:w_start + new_w, :]\n",
    "        return center_crop_video_clip\n",
    "\n",
    "class ClipResize(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, video_clip):\n",
    "        rsz_video_clip = []\n",
    "        new_h, new_w = self.output_size\n",
    "        for frame in video_clip:\n",
    "            rsz_frame = cv2.resize(frame, (new_w, new_h))\n",
    "            rsz_video_clip.append(rsz_frame)\n",
    "        return np.array(rsz_video_clip)\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"\n",
    "    change input channel\n",
    "    D x H x W x C ---> C x D x H x w\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ToTensor, self).__init__()\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        video_clip = sample\n",
    "        video_clip = np.transpose(video_clip, (3, 0, 1, 2))\n",
    "        return video_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27143a1",
   "metadata": {},
   "source": [
    "Note that dataset_json_file must have the format like this:\n",
    "\n",
    "```\n",
    "{\n",
    " \"data\": [\n",
    "  {\n",
    "   \"video_id\": \"--00W1lcxW-WU_40.000\",\n",
    "   \"wav\": \"./sample_audio/00W1lcxW-WU_40.000.wav\",\n",
    "   \"video_path\": \"./sample_frames/00W1lcxW-WU_40.000/\",\n",
    "   \"labels\": \"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\n",
    "  },\n",
    "  {\n",
    "    \"video_id\": \"--KlsG1EnBEjc_000361\",\n",
    "    \"wav\": \"./sample_audio/KlsG1EnBEjc_000361.wav\",\n",
    "    \"video_path\": \"./sample_frames/KlsG1EnBEjc_000361/\",\n",
    "    \"labels\": \"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\n",
    "   }\n",
    "  \n",
    " ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b80ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, dataset_json_file, clip_len = 16, transforms_=None, color_jitter_=None):\n",
    "        # load dataset\n",
    "        self.datapath = dataset_json_file\n",
    "        with open(dataset_json_file, 'r') as fp:\n",
    "            data_json = json.load(fp)\n",
    "        self.data = data_json['data']\n",
    "        self.data = self.pro_data(self.data)\n",
    "        self.num_samples = self.data.shape[0]\n",
    "        print('Dataset has {:d} samples'.format(self.num_samples))\n",
    "        \n",
    "        # some parameters\n",
    "        self.clip_len = clip_len\n",
    "        self.toPIL = transforms.ToPILImage()\n",
    "        self.transforms_ = transforms_\n",
    "        self.color_jitter_ = color_jitter_\n",
    "        \n",
    "        \n",
    "    # change python list to numpy array to avoid memory leak.\n",
    "    def pro_data(self, data_json):\n",
    "        for i in range(len(data_json)):\n",
    "            data_json[i] = [data_json[i]['wav'], data_json[i]['labels'], data_json[i]['video_id'], data_json[i]['video_path']]\n",
    "        data_np = np.array(data_json, dtype=str)\n",
    "        return data_np\n",
    "    \n",
    "    def decode_data(self, np_data):\n",
    "        datum = {}\n",
    "        datum['wav'] = np_data[0]\n",
    "        datum['labels'] = np_data[1]\n",
    "        datum['video_id'] = np_data[2]\n",
    "        datum['video_path'] = np_data[3]\n",
    "        return datum\n",
    "    \n",
    "    def _loop_load_rgb(self, video_path, clip_len):\n",
    "        video_clip = []\n",
    "        for i in range(clip_len):\n",
    "            cur_img_path = os.path.join(video_path, \"frame_\" + \"{:02}.jpg\".format(i))\n",
    "            img = cv2.imread(cur_img_path)\n",
    "            video_clip.append(img)\n",
    "        video_clip = np.array(video_clip)\n",
    "        return video_clip\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        datum = self.data[index]\n",
    "        datum = self.decode_data(datum) # to jason file\n",
    "        video_id, video_path = datum['video_id'],datum['video_path'] \n",
    "        rgb_clip = self._loop_load_rgb(video_path, self.clip_len)\n",
    "        \n",
    "        if not self.transforms_ == None:\n",
    "            trans_clip = self.transforms_(rgb_clip)\n",
    "        else:\n",
    "            trans_clip = rgb_clip  \n",
    "        trans_clip_cj = trans_clip\n",
    "        ## apply different color jittering for each frame in the video clip\n",
    "        #trans_clip_cj = []\n",
    "        #for frame in trans_clip:\n",
    "            #frame = self.toPIL(frame)  # PIL image\n",
    "            #frame = self.color_jitter_(frame)  # tensor [C x H x W]\n",
    "            #frame = np.array(frame)\n",
    "            #trans_clip_cj.append(frame)\n",
    "        trans_clip_cj = np.array(trans_clip_cj).transpose(3, 0, 1, 2)\n",
    "        return trans_clip_cj\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10627bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2 samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 16, 224, 224)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = VideoDataset(args.data_train, clip_len = 16)\n",
    "temp = iter(dataset_train)\n",
    "next(temp).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db6c29",
   "metadata": {},
   "source": [
    "### 2.2 one simple test to load video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e0601a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = \"--00W1lcxW-WU_40.000\"\n",
    "video_path = \"./sample_frames/00W1lcxW-WU_40.000/\"\n",
    "clip_len = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "977e021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_load_rgb(video_path, clip_len):\n",
    "    video_clip = []\n",
    "    for i in range(clip_len):\n",
    "        cur_img_path = os.path.join(video_path, \"frame_\" + \"{:02}.jpg\".format(i))\n",
    "        img = cv2.imread(cur_img_path)\n",
    "        video_clip.append(img)\n",
    "    video_clip = np.array(video_clip)\n",
    "    return video_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc344604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 224, 224, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_clip = loop_load_rgb(video_path, clip_len)\n",
    "video_clip.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c154eef",
   "metadata": {},
   "source": [
    "# 3. Audio-Video dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70b14748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioVideoDataset(Dataset):\n",
    "    def __init__(self, dataset_json_file, audio_conf, label_csv=None, train_model = True,\n",
    "                 clip_len = 16, transforms_=None, color_jitter_=None):\n",
    "        # load dataset\n",
    "        \n",
    "        self.datapath = dataset_json_file\n",
    "        with open(dataset_json_file, 'r') as fp:\n",
    "            data_json = json.load(fp)\n",
    "        self.data = data_json['data']\n",
    "        self.data = self.pro_data(self.data)\n",
    "        self.num_samples = self.data.shape[0]\n",
    "        print('Dataset has {:d} samples'.format(self.num_samples))\n",
    "        \n",
    "        # some parameters for audio\n",
    "        self.audio_conf = audio_conf\n",
    "        self.melbins = self.audio_conf.get('num_mel_bins')\n",
    "        self.target_length = self.audio_conf.get('target_length')\n",
    "        self.train_model = train_model\n",
    "        self.norm_mean = self.audio_conf.get('mean')\n",
    "        self.norm_std = self.audio_conf.get('std')\n",
    "        self.freqm = self.audio_conf.get('freqm', 0)\n",
    "        self.timem = self.audio_conf.get('timem', 0)\n",
    "        print('now using following mask: {:d} freq, {:d} time'.format(self.audio_conf.get('freqm'), self.audio_conf.get('timem')))\n",
    "        \n",
    "        \n",
    "        # some parameters for video\n",
    "        self.clip_len = clip_len\n",
    "        self.toPIL = transforms.ToPILImage()\n",
    "        self.transforms_ = transforms_\n",
    "        self.color_jitter_ = color_jitter_\n",
    "        \n",
    "        # if add noise for data augmentation\n",
    "        self.noise = self.audio_conf.get('noise', False)\n",
    "        if self.noise == True:\n",
    "            print('now use noise augmentation')\n",
    "        else:\n",
    "            print('not use noise augmentation')\n",
    "        \n",
    "        # skip_norm is a flag that if you want to skip normalization to compute the normalization stats using src/get_norm_stats.py, if Ture, input normalization will be skipped for correctly calculating the stats.\n",
    "        # set it as True ONLY when you are getting the normalization stats.\n",
    "        self.skip_norm = self.audio_conf.get('skip_norm') if self.audio_conf.get('skip_norm') else False\n",
    "        if self.skip_norm:\n",
    "            print('now skip normalization (use it ONLY when you are computing the normalization stats).')\n",
    "        else:\n",
    "            print('use dataset mean {:.3f} and std {:.3f} to normalize the input.'.format(self.norm_mean, self.norm_std))\n",
    "    \n",
    "    # change python list to numpy array to avoid memory leak.\n",
    "    def pro_data(self, data_json):\n",
    "        for i in range(len(data_json)):\n",
    "            data_json[i] = [data_json[i]['wav'], data_json[i]['labels'], data_json[i]['video_id'], data_json[i]['video_path']]\n",
    "        data_np = np.array(data_json, dtype=str)\n",
    "        return data_np\n",
    "    \n",
    "    def decode_data(self, np_data):\n",
    "        datum = {}\n",
    "        datum['wav'] = np_data[0]\n",
    "        datum['labels'] = np_data[1]\n",
    "        datum['video_id'] = np_data[2]\n",
    "        datum['video_path'] = np_data[3]\n",
    "        return datum\n",
    "    \n",
    "    def _wav2fbank(self, filename):\n",
    "        waveform, sr = torchaudio.load(filename)\n",
    "        waveform = waveform - waveform.mean()\n",
    "        try:\n",
    "            fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False, window_type='hanning', num_mel_bins=self.melbins, dither=0.0, frame_shift=10)\n",
    "        except:\n",
    "            fbank = torch.zeros([512, 128]) + 0.01\n",
    "            print('there is a loading error')  \n",
    "        target_length = self.target_length\n",
    "        n_frames = fbank.shape[0]\n",
    "        p = target_length - n_frames\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "        return fbank\n",
    "    \n",
    "    def _loop_load_rgb(self, video_path, clip_len):\n",
    "        video_clip = []\n",
    "        for i in range(clip_len):\n",
    "            cur_img_path = os.path.join(video_path, \"frame_\" + \"{:02}.jpg\".format(i))\n",
    "            img = cv2.imread(cur_img_path)\n",
    "            video_clip.append(img)\n",
    "        video_clip = np.array(video_clip)\n",
    "        return video_clip\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        datum = self.data[index]\n",
    "        datum = self.decode_data(datum) # to jason file\n",
    "        \n",
    "        # part 1: get audio\n",
    "        try:\n",
    "            fbank = self._wav2fbank(datum['wav'])\n",
    "        except:\n",
    "            fbank = torch.zeros([self.target_length, 128]) + 0.01\n",
    "            print('there is an error in loading audio')\n",
    "\n",
    "        # SpecAug, not do for eval set\n",
    "        if self.train_model: # training mode\n",
    "            freqm = torchaudio.transforms.FrequencyMasking(self.freqm)\n",
    "            timem = torchaudio.transforms.TimeMasking(self.timem)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "            fbank = fbank.unsqueeze(0)\n",
    "            if self.freqm != 0:\n",
    "                fbank = freqm(fbank)\n",
    "            if self.timem != 0:\n",
    "                fbank = timem(fbank)\n",
    "            fbank = fbank.squeeze(0)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "            # normalize the input for both training and test\n",
    "            if self.skip_norm == False:\n",
    "                fbank = (fbank - self.norm_mean) / (self.norm_std)\n",
    "            # skip normalization the input ONLY when you are trying to get the normalization stats.\n",
    "            else:\n",
    "                pass\n",
    "            # if add noise for data augmentation\n",
    "            #if (self.noise == True) and (self.train_model == True):\n",
    "            if self.noise == True :\n",
    "                fbank = fbank + torch.rand(fbank.shape[0], fbank.shape[1]) * np.random.rand() / 10\n",
    "                fbank = torch.roll(fbank, np.random.randint(-self.target_length, self.target_length), 0)\n",
    "        \n",
    "        else: #evaluation mode\n",
    "            fbank = fbank.squeeze(0)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "\n",
    "            # normalize the input for both training and test\n",
    "            if self.skip_norm == False:\n",
    "                fbank = (fbank - self.norm_mean) / (self.norm_std)\n",
    "            # skip normalization the input ONLY when you are trying to get the normalization stats.\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # part 2: get video\n",
    "        \n",
    "        video_id, video_path = datum['video_id'],datum['video_path'] \n",
    "        rgb_clip = self._loop_load_rgb(video_path, self.clip_len)\n",
    "        \n",
    "        if not self.transforms_ == None:\n",
    "            trans_clip = self.transforms_(rgb_clip)\n",
    "        else:\n",
    "            trans_clip = rgb_clip  \n",
    "        trans_clip_cj = trans_clip\n",
    "        ## apply different color jittering for each frame in the video clip\n",
    "        #trans_clip_cj = []\n",
    "        #for frame in trans_clip:\n",
    "            #frame = self.toPIL(frame)  # PIL image\n",
    "            #frame = self.color_jitter_(frame)  # tensor [C x H x W]\n",
    "            #frame = np.array(frame)\n",
    "            #trans_clip_cj.append(frame)\n",
    "        trans_clip_cj = np.array(trans_clip_cj).transpose(3, 0, 1, 2)\n",
    "        return fbank, trans_clip_cj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e7997f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2 samples\n",
      "now using following mask: 0 freq, 0 time\n",
      "not use noise augmentation\n",
      "use dataset mean -4.268 and std 4.569 to normalize the input.\n"
     ]
    }
   ],
   "source": [
    "dataset_train = AudioVideoDataset(args.data_train, clip_len = 16,audio_conf=audio_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d4236fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 128]), (3, 16, 224, 224))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = iter(dataset_train)\n",
    "fbank, trans_clip_cj = next(temp)\n",
    "fbank.shape, trans_clip_cj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78f51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
